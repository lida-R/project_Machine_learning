# machine-learning-Final project


# first download the data from the links referenced and then upload the data files into R ###

setwd("D:/Lid/Machine learning/exam")
training <- read.csv("pml-training.csv", na.strings = c("NA", "#DIV/0!", ""))
testing  <- read.csv("pml-testing.csv",  na.strings = c("NA", "#DIV/0!", ""))
dim(training); dim(testing)
summary(training) ; str(training)
table(training$classe) 
###take a look at the data and particularly at classe which is the variable we need to predict
prop.table(table(training$user_name, training$classe), 1)
prop.table(table(training$classe))

##Based on the above information,first do some basic data clean-up by removing columns 1 to 6, which are just for information and reference purposes###
training <- training[, 7:160]
testing  <- testing[, 7:160]

###  removing all columns that are mostly NA:
NotNA  <- apply(!is.na(training), 2, sum) > 19621  
training <- training[, NotNA]
testing  <- testing[, NotNA]
dim(training); dim(testing)

## training purposes (actual model building), while the 40% remainder will be used only for testing, evaluation and accuracy measurement##
install.packages("caret")
library(caret)

## Loading required package: lattice, Loading required package: ggplot2
library(lattice)
library(ggplot2)

set.seed(3141592)
###train1 is the training data set (it contains 11776 observations, or about 60% of the entire training data set),
####and test1 is the testing data set (it contains 7846 observations about 40% of the entire training data set)###

inTrain <- createDataPartition(y=training$classe, p=0.60, list=FALSE)
train1  <- training[inTrain,]
test1  <- training[-inTrain,]
dim(train1); dim(test1)


## identify the "zero covariates" from train1 and then remove these "zero covariates" from both train1 and test1##
nzero_co <- nearZeroVar(train1)
if(length(nzero_co) > 0) {
  train1 <- train1[, -nzero_co]
  test1 <- test1[, -nzero_co]
}
dim(train1); dim(test1)

##############fitting the model################

install.packages("randomForest")

library(randomForest)

set.seed(3141592)

fitModel <- randomForest(classe~., data=train1, importance=TRUE, ntree=100)
varImpPlot(fitModel)


##### Using the Accuracy and Gini graphs above, I selected the top 10 variables that I'll use for model building.
##If the accuracy of the resulting model is acceptable, limiting the number of variables is a good idea to ensure readability and interpretability of the model.
##A model with 10 parameters is certainly much more user friendly than a model with 53 parameters.################

### By calculation the correlation matrix for these 10 variables and replace the 1s in the diagonal with 0s, and outputs which variables have an absolute value correlation above 75% ,
## we may have problem with roll_belt and yaw_belt which have a high correlation (above 75%) with each other ###

cor_mat = cor(train1[,c("yaw_belt","roll_belt","num_window","pitch_belt","magnet_dumbbell_z","magnet_dumbbell_y","pitch_forearm","accel_dumbbell_y","roll_arm","roll_forearm")])
diag(cor_mat) <- 0
which(abs(cor_mat)>0.75, arr.ind=TRUE)



cor(train1$roll_belt, train1$yaw_belt)

#####But these two variables are on top of the Accuracy and Gini graphs, and it may seem scary to eliminate one of them. but we eliminate yaw_belt from the list of 10 variables and concentrate only on the remaining 9 variables###

### By re-running the correlation script above (eliminating yaw_belt) and outputting max(correl), the maximum correlation among these 9 variables is 50.57% so we are satisfied with this choice of relatively independent set of covariates. Now there is an interesting relationship between roll_belt and magnet_dumbbell_y. ###

qplot(roll_belt, magnet_dumbbell_y, colour=classe, data=train1)

### This graph suggests that we could probably categorize the data into groups based on roll_belt values and a quick tree classifier selects roll_belt as the first discriminant among all 53 covariates
##(which explains why we have eliminated yaw_belt instead of roll_belt, and not the opposite: it is a "more important" covariate)####

install.packages("rpart.plot")

library(rpart.plot)

fitModel <- rpart(classe~., data=train1, method="class")
prp(fitModel)

############### Now create my model by using a Random Forest algorithm and train() funstion from the caret package###
#### By using 9 variables out of the 53 as model parameters. These variables were among the most significant variables generated by an initial Random Forest algorithm, and are roll_belt, num_window, pitch_belt, magnet_dumbbell_y, magnet_dumbbell_z, pitch_forearm, accel_dumbbell_y, roll_arm, and roll_forearm. These variable are relatively independent as the maximum correlation among them is 50.57%.####

set.seed(3141592)
install.packages("e1071")
library(e1071)

fitModel <- train(classe~roll_belt+num_window+pitch_belt+magnet_dumbbell_y+magnet_dumbbell_z+pitch_forearm+accel_dumbbell_y+roll_arm+roll_forearm,
                  data=train1,
                  method="rf",
                  trControl=trainControl(method="cv",number=2),
                  prox=TRUE,
                  verbose=TRUE,
                  allowParallel=TRUE)
fitModel
saveRDS(fitModel, "modelRF.Rds")
fitModel <- readRDS("modelRF.Rds")

## the accuracy of this model##############

predictions_RF <- predict(fitModel, newdata=test1)
confusionMat <- confusionMatrix(predictions_RF, test1$classe)
confusionMat

######### 99.77% is the number of accuracy which totally validates the hypothesis made to eliminate most variables and use only 9 relatively independent covariates.##########

###########################  Estimation of the out-of-sample error rate ######################

### The test1 test set was removed and left untouched during variable selection, training and optimizing of the Random Forest algorithm. 
##Therefore this testing subset gives an unbiased estimate of the Random Forest algorithm's prediction accuracy (99.77% as calculated above).
##The Random Forest's out-of-sample error rate is derived by the formula 100% - Accuracy = 0.23%, or can be calculated directly by the following lines of code:### 

missClass = function(values, predicted) {
  sum(predicted != values) / length(values)
}
OOS_errRate = missClass(test1$classe, predictions_RF)
OOS_errRate

## [1] 0.002294163  The out-of-sample error rate is 0.22%.###################

###############  predict the classification of the 20 observations of the testing data set ###############

predictions_RF <- predict(fitModel, newdata=testing)
testing$classe <- predictions_RF

#### Create one .CSV file with all the results, presented in two columns (named problem_id and classe) and 20 rows of data#####

submit <- data.frame(problem_id = testing$problem_id, classe = predictions_RF)
write.csv(submit, file = "coursera-submission.csv", row.names = FALSE)

##### Create twenty .TXT file that we will upload one by one in the Coursera website (the 20 files created are called problem_1.txt to problem_20.txt)##########

answers = testing$classe
write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_",i,".txt")
    write.table(x[i], file=filename, quote=FALSE, row.names=FALSE, col.names=FALSE)
  }
}
write_files(answers)
